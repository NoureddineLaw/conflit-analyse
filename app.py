"""
âš–ï¸ Conflit Analyse â€“ Ù…Ø­Ù„Ù„ ØªÙ†Ø§Ø²Ø¹ Ø§Ù„Ù‚ÙˆØ§Ù†ÙŠÙ† Ø§Ù„Ø¯ÙˆÙ„ÙŠ Ø§Ù„Ø®Ø§Øµ
Ù…Ø´Ø±ÙˆØ¹ Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ Ù…ÙˆØ¬Ù‡ Ù„Ø·Ù„Ø¨Ø© Ø§Ù„Ø³Ù†Ø© Ø§Ù„Ø«Ø§Ù„Ø«Ø© Ù‚Ø§Ù†ÙˆÙ† Ø®Ø§Øµ
Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø·Ø§Ù„Ø¨: Øª. Ù†ÙˆØ±Ø§Ù„Ø¯ÙŠÙ† â€“ Ø¬Ø§Ù…Ø¹Ø© Ø§Ù„Ù†Ø¹Ø§Ù…Ø©ØŒ Ø§Ù„Ø¬Ø²Ø§Ø¦Ø±
"""

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# ğŸ”¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø§Ù„Ù…ÙØªÙˆØ­ Ø§Ù„Ù…ØµØ¯Ø±
MODEL_NAME = "FreedomIntelligence/AceGPT-7B"

print("â³ Ø¬Ø§Ø±Ù ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬... Ù‚Ø¯ ÙŠØ³ØªØºØ±Ù‚ Ø¨Ø¶Ø¹ Ø¯Ù‚Ø§Ø¦Ù‚.")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float16,
    device_map="auto"
)
print("âœ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¬Ø§Ù‡Ø²!")

PROMPT_TEMPLATE = """
Ø£Ù†Øª Ø£Ø³ØªØ§Ø° ÙÙŠ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ø¯ÙˆÙ„ÙŠ Ø§Ù„Ø®Ø§Øµ Ù…ÙˆØ¬Ù‡ Ù„Ø·Ù„Ø¨Ø© Ø§Ù„Ø³Ù†Ø© Ø§Ù„Ø«Ø§Ù„Ø«Ø© Ù‚Ø§Ù†ÙˆÙ† Ø®Ø§Øµ.
Ø­Ù„Ù‘Ù„ Ø§Ù„ÙˆÙ‚Ø§Ø¦Ø¹ Ø§Ù„ØªØ§Ù„ÙŠØ© ØªØ­Ù„ÙŠÙ„Ø§Ù‹ Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠÙ‹Ø§ ÙˆÙ…Ù†Ù‡Ø¬ÙŠÙ‹Ø§ ÙˆÙÙ‚ Ø§Ù„Ù…ÙˆØ§Ø¯ Ù…Ù† 9 Ø¥Ù„Ù‰ 24 Ù…Ù† Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ù…Ø¯Ù†ÙŠ Ø§Ù„Ø¬Ø²Ø§Ø¦Ø±ÙŠ.

1) Ø§Ù„ØªÙƒÙŠÙŠÙ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ (Ù…9)
2) Ø§Ù„Ø¹Ù†ØµØ± Ø§Ù„Ø£Ø¬Ù†Ø¨ÙŠ
3) Ø¶Ø§Ø¨Ø· Ø§Ù„Ø¥Ø³Ù†Ø§Ø¯
4) Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„ÙˆØ§Ø¬Ø¨ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
5) Ø§Ù„Ø¥Ø­Ø§Ù„Ø© (Ù…23 Ù…ÙƒØ±Ø± 1)
6) Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø¹Ø§Ù… (Ù…24)
7) Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©.

Ø§Ù„ÙˆÙ‚Ø§Ø¦Ø¹: {}
"""

def analyser(faits: str):
    prompt = PROMPT_TEMPLATE.format(faits)
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=350)
    texte = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print("\nğŸ“˜ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ:\n")
    print(texte)

if __name__ == "__main__":
    print("âš–ï¸ Conflit Analyse â€“ Ù…Ø­Ù„Ù„ ØªÙ†Ø§Ø²Ø¹ Ø§Ù„Ù‚ÙˆØ§Ù†ÙŠÙ† Ø§Ù„Ø¯ÙˆÙ„ÙŠ Ø§Ù„Ø®Ø§Øµ")
    faits = input("\nğŸ“ Ø£Ø¯Ø®Ù„ Ø§Ù„ÙˆÙ‚Ø§Ø¦Ø¹ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©:\n> ")
    analyser(faits)
